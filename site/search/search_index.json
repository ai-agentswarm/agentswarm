{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to Agentswarm","text":"<p>Agentswarm is a recursive, functional, and state-isolated Multi-Agent Framework.</p> <p>It is designed to build complex, scalable agentic systems by composing small, focused agents.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Recursion: Agents can be composed of other agents.</li> <li>State Isolation: Each agent execution has its own isolated context.</li> <li>MCP Support: Native integration with the Model Context Protocol (MCP) to connect to external tools.</li> </ul>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Install the library:</p> <pre><code>pip install agentswarm\n</code></pre> <p>Create a simple agent:</p> <pre><code>from agentswarm.agents import ReActAgent\n\nclass MyAgent(ReActAgent):\n    # ... implementation ...\n    pass\n</code></pre> <p>Explore the documentation sections to learn more about the core concepts, available agents, and how to extend the framework.</p>"},{"location":"agents/base/","title":"Base Agent","text":"<p>The <code>BaseAgent</code> is the fundamental unit of work in Agentswarm.</p>"},{"location":"agents/base/#the-abstract-concept","title":"The Abstract Concept","text":"<p>An Agent in Agentswarm is simply a class that: 1.  Accepts Input: Defined by a Pydantic model (<code>InputType</code>). 2.  Performs Work: The <code>execute</code> method, which can do anything\u2014call an LLM, query a database, or run a calculation. 3.  Produces Output: Defined by a Pydantic model (<code>OutputType</code>). 4.  Describes Itself: Provides a name (<code>id</code>) and <code>description</code> so that other agents (specifically LLMs) know how to use it.</p>"},{"location":"agents/base/#implementing-a-new-agent","title":"Implementing a New Agent","text":"<p>To create a custom agent, inherit from <code>BaseAgent</code> and specify your input/output types.</p> <pre><code>from pydantic import BaseModel\nfrom agentswarm.agents import BaseAgent\nfrom agentswarm.datamodels import Context\n\nclass MyInput(BaseModel):\n    query: str\n\nclass MyAgent(BaseAgent[MyInput, str]): \n    # Specifying [InputType, OutputType] generics is CRITICAL \n    # for automatic schema generation.\n\n    def id(self) -&gt; str:\n        return \"my_agent\"\n\n    def description(self, user_id: str) -&gt; str:\n        return \"Helps the user with X\"\n\n    async def execute(self, user_id: str, context: Context, input: MyInput = None) -&gt; str:\n        # Your logic here\n        return f\"Processed {input.query}\"\n</code></pre>"},{"location":"agents/base/#standard-outputs","title":"Standard Outputs","text":"<p>Agentswarm provides a set of predefined output models in <code>agentswarm.datamodels.responses</code>. Using these standard responses helps maintain consistency across your agents, especially when they are used as tools by a ReAct agent.</p> <ul> <li><code>VoidResponse</code>: Use when the agent performs an action but returns no data (e.g., \"Email sent\").</li> <li><code>KeyStoreResponse</code>: Use when the agent stores a large object in the Store and returns only the key and a description. This prevents context pollution.</li> <li><code>ThoughtResponse</code>: Used internally for reasoning steps.</li> </ul>"},{"location":"agents/base/#example","title":"Example","text":"<pre><code>from agentswarm.datamodels import VoidResponse\n\nclass EmailAgent(BaseAgent[EmailInput, VoidResponse]):\n    # ...\n    async def execute(self, user_id, context, input):\n        send_email(input)\n        return VoidResponse()\n</code></pre>"},{"location":"agents/base/#error-handling","title":"Error Handling","text":"<p>Agentswarm follows standard Python idioms for error handling. If an agent encounters a problem that prevents it from completing its task, it should raise an exception.</p> <p>The framework (specifically the <code>ReActAgent</code> or other orchestrators) is designed to catch these exceptions and handle them appropriately\u2014typically by reporting the error back to the LLM so it can decide how to recover (e.g., by trying a different tool or modifying inputs).</p> <pre><code>    async def execute(self, user_id, context, input):\n        if not input.valid:\n             # This will be caught by the calling agent and presented as an error\n             raise ValueError(f\"Invalid input: {input}\")\n        # ...\n</code></pre>"},{"location":"agents/base/#api-reference","title":"API Reference","text":""},{"location":"agents/base/#agentswarm.agents.BaseAgent","title":"<code>agentswarm.agents.BaseAgent</code>","text":"<p>               Bases: <code>Generic[InputType, OutputType]</code></p> Source code in <code>src/agentswarm/agents/base_agent.py</code> <pre><code>class BaseAgent(Generic[InputType, OutputType]):\n\n    @abstractmethod\n    def id(self) -&gt; str:\n        pass\n\n    @abstractmethod\n    def description(self, user_id: str) -&gt; str:\n        pass\n\n    @abstractmethod\n    async def execute(self, user_id: str, context: Context, input: InputType = None) -&gt; OutputType:\n        pass\n\n    def _get_generic_type(self, index: int):\n        \"\"\"\n        Obtains the concrete type of the generic at the specified index (0 for InputType, 1 for OutputType).\n        Works by inspecting __orig_bases__ of the class at runtime.\n        \"\"\"\n        for base in getattr(self.__class__, \"__orig_bases__\", []):\n            origin = getattr(base, \"__origin__\", None)\n            if origin is BaseAgent or issubclass(origin, BaseAgent):\n                args = get_args(base)\n                if args and len(args) &gt; index:\n                    return args[index]\n        return None\n\n    def input_parameters(self) -&gt; dict:\n        input_type = self._get_generic_type(0)\n        if input_type and hasattr(input_type, 'model_json_schema'):\n            schema = input_type.model_json_schema()\n            schema.pop('title', None)\n            return schema\n        return {}\n\n    def output_parameters(self) -&gt; dict:\n        output_type = self._get_generic_type(1)\n        if output_type and hasattr(output_type, 'model_json_schema'):\n            schema = output_type.model_json_schema()\n            schema.pop('title', None)\n            return schema\n        return {}\n</code></pre>"},{"location":"agents/mcp/","title":"Model Context Protocol (MCP) Support","text":"<p>Agentswarm provides native support for the Model Context Protocol, allowing you to connect your agents to a vast ecosystem of external tools and data sources.</p>"},{"location":"agents/mcp/#how-it-works","title":"How it Works","text":"<ol> <li>MCPBaseAgent: Manages the connection to an MCP server (e.g., via stdio).</li> <li>MCPToolAgent: Automatically wraps each tool exposed by the server into a standard <code>BaseAgent</code>.</li> </ol>"},{"location":"agents/mcp/#api-reference","title":"API Reference","text":""},{"location":"agents/mcp/#mcpbaseagent","title":"MCPBaseAgent","text":""},{"location":"agents/mcp/#agentswarm.agents.MCPBaseAgent","title":"<code>agentswarm.agents.MCPBaseAgent</code>","text":"<p>A base class for connecting to an MCP server and discovering tools. This acts as a factory/manager for MCPToolAgents.</p> Source code in <code>src/agentswarm/agents/mcp_agent.py</code> <pre><code>class MCPBaseAgent:\n    \"\"\"\n    A base class for connecting to an MCP server and discovering tools.\n    This acts as a factory/manager for MCPToolAgents.\n    \"\"\"\n\n    def __init__(self):\n        self._session: Optional[ClientSession] = None\n        self._exit_stack = None\n\n    @abstractmethod\n    def get_server_params(self) -&gt; StdioServerParameters:\n        \"\"\"\n        Define the connection parameters for the MCP server.\n        \"\"\"\n        pass\n\n    @asynccontextmanager\n    async def connect(self):\n        \"\"\"\n        Async context manager to establish connection to the MCP server.\n        \"\"\"\n        server_params = self.get_server_params()\n\n        # We manually manage the nested context managers to keep the session alive appropriately\n        async with stdio_client(server_params) as (read, write):\n            async with ClientSession(read, write) as session:\n                self._session = session\n                await session.initialize()\n                yield self\n                self._session = None\n\n    async def get_agents(self) -&gt; List[MCPToolAgent]:\n        \"\"\"\n        Discovers tools on the connected MCP server and returns them as a list of MCPToolAgent.\n        Must be called within the 'connect' context.\n        \"\"\"\n        if not self._session:\n            raise RuntimeError(\"MCP session is not active. Use 'async with agent.connect():'\")\n\n        response = await self._session.list_tools()\n        agents = []\n        for tool in response.tools:\n            agents.append(MCPToolAgent(self._session, tool))\n\n        return agents\n</code></pre>"},{"location":"agents/mcp/#agentswarm.agents.MCPBaseAgent.connect","title":"<code>connect()</code>  <code>async</code>","text":"<p>Async context manager to establish connection to the MCP server.</p> Source code in <code>src/agentswarm/agents/mcp_agent.py</code> <pre><code>@asynccontextmanager\nasync def connect(self):\n    \"\"\"\n    Async context manager to establish connection to the MCP server.\n    \"\"\"\n    server_params = self.get_server_params()\n\n    # We manually manage the nested context managers to keep the session alive appropriately\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            self._session = session\n            await session.initialize()\n            yield self\n            self._session = None\n</code></pre>"},{"location":"agents/mcp/#agentswarm.agents.MCPBaseAgent.get_agents","title":"<code>get_agents()</code>  <code>async</code>","text":"<p>Discovers tools on the connected MCP server and returns them as a list of MCPToolAgent. Must be called within the 'connect' context.</p> Source code in <code>src/agentswarm/agents/mcp_agent.py</code> <pre><code>async def get_agents(self) -&gt; List[MCPToolAgent]:\n    \"\"\"\n    Discovers tools on the connected MCP server and returns them as a list of MCPToolAgent.\n    Must be called within the 'connect' context.\n    \"\"\"\n    if not self._session:\n        raise RuntimeError(\"MCP session is not active. Use 'async with agent.connect():'\")\n\n    response = await self._session.list_tools()\n    agents = []\n    for tool in response.tools:\n        agents.append(MCPToolAgent(self._session, tool))\n\n    return agents\n</code></pre>"},{"location":"agents/mcp/#agentswarm.agents.MCPBaseAgent.get_server_params","title":"<code>get_server_params()</code>  <code>abstractmethod</code>","text":"<p>Define the connection parameters for the MCP server.</p> Source code in <code>src/agentswarm/agents/mcp_agent.py</code> <pre><code>@abstractmethod\ndef get_server_params(self) -&gt; StdioServerParameters:\n    \"\"\"\n    Define the connection parameters for the MCP server.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"agents/mcp/#mcptoolagent","title":"MCPToolAgent","text":""},{"location":"agents/mcp/#agentswarm.agents.MCPToolAgent","title":"<code>agentswarm.agents.MCPToolAgent</code>","text":"<p>               Bases: <code>BaseAgent[dict, Any]</code></p> <p>An agent that wraps a specific tool from an MCP server.</p> Source code in <code>src/agentswarm/agents/mcp_agent.py</code> <pre><code>class MCPToolAgent(BaseAgent[dict, Any]):\n    \"\"\"\n    An agent that wraps a specific tool from an MCP server.\n    \"\"\"\n    def __init__(self, session: ClientSession, tool: Tool):\n        self._session = session\n        self._tool = tool\n\n    def id(self) -&gt; str:\n        return self._tool.name\n\n    def description(self, user_id: str) -&gt; str:\n        return self._tool.description or f\"Tool {self._tool.name} from MCP server\"\n\n    def input_parameters(self) -&gt; dict:\n        # MCP tools define inputSchema in a way compatible with JSON Schema\n        schema = self._tool.inputSchema\n        if schema:\n             # Ensure title is removed if present as per convention in other agents, though optional\n            schema.pop('title', None)\n            return schema\n        return {}\n\n    def output_parameters(self) -&gt; dict:\n        # MCP tools return generic content lists (text, images, etc.)\n        # We don't have a strict schema for the output content structure here yet\n        return {\"type\": \"object\", \"description\": \"The result of the tool execution\"}\n\n    async def execute(self, user_id: str, context: Context, input: dict = None) -&gt; Any:\n        if input is None:\n            input = {}\n\n        # Call the tool on the MCP server\n        result = await self._session.call_tool(self._tool.name, arguments=input)\n\n        # Process result.content which is a list of TextContent | ImageContent | EmbeddedResource\n        # For simplicity, we return the raw list or a simplified text representation\n        # depending on what the framework expects. BaseAgent expects output_type.\n        # Since we defined OutputType as Any, we return the result object or content.\n        return result.content\n</code></pre>"},{"location":"agents/mcp/#example-usage","title":"Example Usage","text":"<pre><code>from agentswarm.agents import MCPBaseAgent\nfrom mcp import StdioServerParameters\n\nclass MyMCP(MCPBaseAgent):\n    def get_server_params(self):\n        return StdioServerParameters(command=\"uvx\", args=[\"mcp-server-sqlite\", ...])\n\nasync def main():\n    factory = MyMCP()\n    async with factory.connect() as agent_manager:\n        tools = await agent_manager.get_agents()\n        # tools is a list of MCPToolAgent instances you can now use!\n</code></pre>"},{"location":"agents/others/","title":"Other Agents","text":"<p>Agentswarm includes several specialized agents designed for functional composition. These agents are typically used as tools within a <code>ReActAgent</code> loop or composed together in a pipeline.</p>"},{"location":"agents/others/#mapreduce-agent","title":"MapReduce Agent","text":"<p>The <code>MapReduceAgent</code> is a powerful tool for scaling operations. It takes a list of items and an \"instructed agent\". It applies the instruction to every item in the list in parallel (Map) and then synthesizes the results (Reduce).</p> <p>Use Case: Summarizing 10 different news articles at once.</p>"},{"location":"agents/others/#agentswarm.agents.MapReduceAgent","title":"<code>agentswarm.agents.MapReduceAgent</code>","text":"<p>               Bases: <code>ReActAgent[MapReduceInput, KeyStoreResponse]</code></p> Source code in <code>src/agentswarm/agents/map_reduce_agent.py</code> <pre><code>class MapReduceAgent(ReActAgent[MapReduceInput, KeyStoreResponse]):\n    def __init__(self, max_iterations: int = 100, agents: List[BaseAgent] = []):\n        super().__init__(max_iterations)\n        self.agents = agents\n\n    def get_llm(self, user_id: str) -&gt; LLM:\n        # TODO: Better LLM consiguration\n        return GeminiLLM(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n    def id(self) -&gt; str:\n        return \"map-reduce\"\n\n    def description(self, user_id: str) -&gt; str:\n        return \"\"\"USE THIS AGENT FOR:\n        1. COMPLEX CONTENT GENERATION: Creating multi-part artifacts like software libraries, full books, or extensive reports where each chapter/module needs to be generated in parallel.\n        2. RECURSIVE TASKS: Exploration of hierarchical structures (directories, org charts) or recursive problem solving.\n        3. LARGE DATASETS: Processing data that exceeds context limits by splitting (Map) and summarizing (Reduce).\n        4. PARALLEL EXECUTION: Any task requiring massive parallel execution of sub-tasks.\n\n        DO NOT USE FOR: Simple, linear tasks or single-file creation that fit in a single context window.\n        \"\"\"\n\n    def prompt(self, user_id: str) -&gt; str:\n        return \"\"\"You are a recursive Map-Reduce agent. Your GOAL is to solve the given 'task' completely and return the FINAL RESULT.\n\n        ALGORITHM:\n        1. ANALYZE: Understand the task (e.g., analyze a folder).\n        2. MAP: Break it down into sub-tasks (e.g., read files in current dir, recursively call 'map-reduce' for sub-directories).\n        3. EXECUTE: Run these sub-tasks using the available tools. Parallelize where possible.\n        4. REDUCE: Collect ALL results from the sub-tasks.\n        5. SYNTHESIZE: Combine them into a single, comprehensive final report.\n\n        CRITICAL RULES:\n        - ANTI-RECURSION: If your assigned task is T, DO NOT call 'map-reduce' with task T again. You must BREAK DOWN task T into smaller sub-tasks (t1, t2...) and delegate THOSE.\n        - You are the one responsible for executing the atomic actions for T (e.g., gathering data, listing items) before delegating sub-parts.\n        - Do NOT return \"I am working on it\". You must work on it UNTIL IT IS DONE.\n        - Do NOT return partial results unless you have hit a hard limit.\n        - Your final 'assistant' message MUST contain the complete answer/report.\n        \"\"\"\n\n    def available_agents(self, user_id: str) -&gt; List[BaseAgent]:\n        # IMPORTANT: Return a NEW instance of MapReduceAgent instead of self.\n        return [MapReduceAgent(max_iterations=self.max_iterations, agents=self.agents)] + self.agents\n\n    def generate_messages_context(self, user_id: str, context: Context, input: MapReduceInput = None) -&gt; List[Message]:\n        msgs = super().generate_messages_context(user_id, context, input)\n        msgs.append(Message(type=\"user\", content=f\"CURRENT TASK: {input.task}\\n\\nWARNING: Do not call 'map-reduce' with this exact same task '{input.task}' again, as it would cause an infinite loop. decompose it into smaller sub-tasks.\"))\n        return msgs\n</code></pre>"},{"location":"agents/others/#gathering-agent","title":"Gathering Agent","text":"<p>The <code>GatheringAgent</code> is designed to extract information present in the Context. Unlike other agents that might search externally or generate new content, this agent focuses on reading the message history and extracting specific structured data.</p> <p>Use Case: Extracting a user's name and email from a long conversation history.</p>"},{"location":"agents/others/#agentswarm.agents.GatheringAgent","title":"<code>agentswarm.agents.GatheringAgent</code>","text":"<p>               Bases: <code>BaseAgent[GatheringAgentInput, str]</code></p> Source code in <code>src/agentswarm/agents/gathering_agent.py</code> <pre><code>class GatheringAgent(BaseAgent[GatheringAgentInput, str]):\n    def id(self) -&gt; str:\n        return \"gathering-agent\"\n\n    def description(self, user_id: str) -&gt; str:\n        return \"\"\"\nI'm able to gather an information from the store and add to the current context. \nUSE THIS AGENT ONLY WHEN YOU NEED TO DISPLAY THE INFORMATION TO THE USER.\nWhenever possible, if you need to filter or process the data, use the \"transformer-agent\".\n        \"\"\"\n\n    async def execute(self, user_id: str, context: Context, input: GatheringAgentInput) -&gt; str:\n        if not context.store.has(input.key):\n            raise Exception(f\"Information from the store with key {input.key} not found\")\n        value = context.store.get(input.key)\n        return value\n</code></pre>"},{"location":"agents/others/#merge-agent","title":"Merge Agent","text":"<p>The <code>MergeAgent</code> is a simple utility to combine multiple text inputs or results into a single coherent response.</p> <p>Use Case: Combining the results of three different search queries into a single paragraph.</p>"},{"location":"agents/others/#agentswarm.agents.MergeAgent","title":"<code>agentswarm.agents.MergeAgent</code>","text":"<p>               Bases: <code>BaseAgent[MergeAgentInput, KeyStoreResponse]</code></p> Source code in <code>src/agentswarm/agents/merge_agent.py</code> <pre><code>class MergeAgent(BaseAgent[MergeAgentInput, KeyStoreResponse]):\n    def id(self) -&gt; str:\n        return \"merge-agent\"\n\n    def description(self, user_id: str) -&gt; str:\n        return \"I'm able to merge different informations into a single one.\"\n\n    async def execute(self, user_id: str, context: Context, input: MergeAgentInput) -&gt; KeyStoreResponse:\n        values = [context.store.get(key) for key in input.keys]\n        value = \"\\n\".join(values)\n        key = f\"merged_{uuid.uuid4()}\"\n        context.store.set(key, value)\n        return KeyStoreResponse(key=key, description=f\"Merged information from keys {input.keys}\")\n</code></pre>"},{"location":"agents/others/#transformer-agent","title":"Transformer Agent","text":"<p>The <code>TransformerAgent</code> transforms data from one format to another (e.g., JSON to CSV, or text to structured data) based on natural language instructions.</p> <p>Use Case: Converting a messy text list into a clean JSON array.</p>"},{"location":"agents/others/#agentswarm.agents.TransformerAgent","title":"<code>agentswarm.agents.TransformerAgent</code>","text":"<p>               Bases: <code>BaseAgent[TransformerAgentInput, KeyStoreResponse]</code></p> Source code in <code>src/agentswarm/agents/transformer_agent.py</code> <pre><code>class TransformerAgent(BaseAgent[TransformerAgentInput, KeyStoreResponse]):\n    def id(self) -&gt; str:\n        return \"transformer-agent\"\n\n    def description(self, user_id: str) -&gt; str:\n        return \"\"\"\nI'm able to transform an information from the store into a new information in the store.\nI can be used to apply complex llm-based task to the stored data, in order to optimize the general context.\n        \"\"\"\n\n    async def execute(self, user_id: str, context: Context, input: TransformerAgentInput) -&gt; KeyStoreResponse:\n        value = context.store.get(input.key)\n        all = [Message(type=\"user\", content=f\"{value}\")]\n\n        all.append(Message(type=\"user\", content=f\"Filter the previous data using this command:\\n{input.cmd}\\n. The ouput should be a new data, not a prompt or a python code. If not specified, you can optimize the output for your internal use.\"))\n\n        llm = context.default_llm\n        if llm is None:\n            raise ValueError(\"Default LLM not set\")\n        response = await llm.generate(all)\n        context.add_usage(response.usage)\n\n        new_key = f\"transformer_{uuid.uuid4()}\"\n        context.store.set(new_key, response.text)\n\n        return KeyStoreResponse(key=new_key, description=f\"Transformed information from key {input.key} with command {input.cmd}\")\n</code></pre>"},{"location":"agents/others/#thinking-agent","title":"Thinking Agent","text":"<p>The <code>ThinkingAgent</code> is unique. It is not usually meant to be called by the user directly. Instead, it is injected into the <code>ReActAgent</code> loop to allow the LLM to \"think out loud\" before taking actions.</p> <p>It enables the Chain of Thought prompting technique within the agent execution flow.</p>"},{"location":"agents/others/#agentswarm.agents.ThinkingAgent","title":"<code>agentswarm.agents.ThinkingAgent</code>","text":"<p>               Bases: <code>BaseAgent[ThinkingInput, ThoughtResponse]</code></p> Source code in <code>src/agentswarm/agents/thinking_agent.py</code> <pre><code>class ThinkingAgent(BaseAgent[ThinkingInput, ThoughtResponse]):\n    def id(self) -&gt; str:\n        return \"thinking_tool\"\n\n    def description(self, user_id: str) -&gt; str:\n        return (\n            \"Use this tool to plan your actions. \"\n            \"You MUST call this tool AND ALL other necessary action tools IN THE SAME TURN. \"\n            \"Do not wait for the next turn to act. \"\n            \"Plan for parallel execution where possible.\"\n        )\n\n    async def execute(self, user_id: str, context: Context, input_args: ThinkingInput) -&gt; ThoughtResponse:\n        part = \"\"\n        if input_args.self_correction:\n            part += f\" Self-correction: {input_args.self_correction}\"\n        return ThoughtResponse(thought=f\"{input_args.reasoning}.{part}\")\n</code></pre>"},{"location":"agents/react/","title":"ReAct Agent","text":"<p>The <code>ReActAgent</code> is the most powerful agent type in the framework, implementing the Reasoning and Acting loop.</p>"},{"location":"agents/react/#how-it-works","title":"How it Works","text":"<p>The ReAct agent operates in a loop: 1.  Context Construction: It gathers the conversation history and a special System Prompt that defines its operational rules (parallel execution, thinking first, etc.). 2.  Tool Discovery: It looks at the list of <code>available_agents</code> (tools) and converts them into function definitions for the LLM. 3.  Generation: It sends the context to the LLM. 4.  Execution:     *   If the LLM calls the Thinking Tool (parallel to others), the thought is recorded.     *   If the LLM calls other tools, they are executed in parallel (up to a concurrency limit). 5.  Recursion: The results are added back to the context, and the loop repeats until the LLM produces a final answer or the max iterations are reached.</p>"},{"location":"agents/react/#implementation-details","title":"Implementation Details","text":"<p>When you subclass <code>ReActAgent</code>, you typically only need to define: - <code>get_llm()</code>: Which LLM to use. - <code>available_agents()</code>: Which tools (other BaseAgents) this agent can use.</p> <pre><code>class MyOrchestrator(ReActAgent):\n    def get_llm(self, user_id: str):\n        return GeminiLLM()\n\n    def available_agents(self, user_id: str):\n        return [\n            SearchAgent(),\n            DatabaseAgent(),\n            # ...\n        ]\n</code></pre>"},{"location":"agents/react/#api-reference","title":"API Reference","text":""},{"location":"agents/react/#agentswarm.agents.ReActAgent","title":"<code>agentswarm.agents.ReActAgent</code>","text":"<p>               Bases: <code>BaseAgent[InputType, OutputType]</code></p> Source code in <code>src/agentswarm/agents/react_agent.py</code> <pre><code>class ReActAgent(BaseAgent[InputType, OutputType]):\n\n    def __init__(self, max_iterations: int = 100, max_concurrent_agents: int = 5):\n        self.max_iterations = max_iterations\n        self.max_concurrent_agents = max_concurrent_agents\n\n    @abstractmethod\n    def get_llm(self, user_id: str) -&gt; LLM:\n        pass\n\n    @abstractmethod\n    def prompt(self, user_id: str) -&gt; str:\n        pass\n\n    def get_thinking_agent(self):\n        return ThinkingAgent()\n\n    def get_default_agents(self) -&gt; List[BaseAgent]:\n        return [self.get_thinking_agent(), GatheringAgent(), TransformerAgent(), MergeAgent()]\n\n    @abstractmethod\n    def available_agents(self, user_id: str) -&gt; List[BaseAgent]:\n        pass\n\n    def generate_function_calls(self, user_id: str) -&gt; List[LLMFunction]:\n        functions = []\n        for agent in self.available_agents(user_id):\n            try:\n                functions.append(LLMFunction(name=agent.id(), description=agent.description(user_id), parameters=agent.input_parameters()))\n            except Exception as e:\n                print(f\"Error generating function call for agent {agent.id()}: {e}\")\n                print(traceback.format_exc())\n        return functions\n\n    async def agent_execution(self, user_id: str, context: Context, function: LLMFunction):\n        agent = next((agent for agent in self.available_agents(user_id) if agent.id() == function.name), None)\n        if agent is None:\n            raise Exception(f\"Agent {function.name} not found\")\n\n        input_type = agent._get_generic_type(0)\n        if input_type and isinstance(function.arguments, dict):\n            validated_input = input_type(**function.arguments)\n\n            # Create a new context for the agent to support tracing hierarchy\n            new_context = context.copy_for_execution()\n\n            # Trace the agent execution\n            context.tracing.trace_agent(new_context, agent.id(), function.arguments)\n\n            try:\n                result = await agent.execute(user_id, new_context, validated_input)\n                context.tracing.trace_agent_result(new_context, agent.id(), result)\n                return result\n            except Exception as e:\n                context.tracing.trace_agent_error(new_context, agent.id(), e)\n                raise e\n        raise Exception(f\"Invalid arguments for agent {function.name}\")\n\n    def generate_messages_context(self, user_id: str, context: Context, input: InputType = None) -&gt; List[Message]:\n        all: List[Message] = []\n        all.append(Message(type=\"system\", content=REACT_SYS_PROMPT))\n        all.append(Message(type=\"system\", content=f\"Use the tool '{self.get_thinking_agent().id()}' IN PARALLEL to the other tools to explain your plan.\"))\n        all.append(Message(type=\"system\", content=self.prompt(user_id)))\n        all.extend(context.messages)\n        return all\n\n    async def gather_with_concurrency(self, n, *tasks):\n        \"\"\"\n        Runs tasks with a concurrency limit of n.\n        \"\"\"\n        semaphore = asyncio.Semaphore(n)\n\n        async def sem_task(task):\n            async with semaphore:\n                return await task\n\n        return await asyncio.gather(*(sem_task(task) for task in tasks))\n\n    async def execute(self, user_id: str, context: Context, input: InputType = None) -&gt; OutputType:\n\n        current_context = self.generate_messages_context(user_id, context, input)\n        iteration = 0\n        from_prev_iteration = False\n\n        while iteration &lt; self.max_iterations:\n            print(f\"Iteration {iteration}\")\n\n            # Create an iteration step ID\n            iteration_step_id = f\"{context.step_id}_iter_{iteration}\"\n\n            # Trace the iteration start\n            iter_context = context.copy_for_iteration(iteration_step_id, current_context)\n\n            context.tracing.trace_loop_step(iter_context, f\"Iteration {iteration}\")\n\n            tmp_context = current_context\n            if from_prev_iteration:\n                tmp_context = tmp_context + [Message(type=\"user\", content=\"Elaborate the results of the agents\")]\n\n\n            response = await self.get_llm(user_id).generate(tmp_context, functions=self.generate_function_calls(user_id))\n            iter_context.add_usage(response.usage)\n\n            if response.function_calls is None or len(response.function_calls) == 0:\n                return [Message(type=\"assistant\", content=response.text)]\n\n            has_execution_tool = False\n            output = []\n\n            # Prepare tasks for parallel execution\n            tasks = []\n\n            for function_call in response.function_calls:\n                if function_call.name != self.get_thinking_agent().id():\n                    has_execution_tool = True\n\n                # We wrap the execution in a task, capturing the necessary context\n                task = self.execute_and_handle_result(user_id, iter_context, function_call, context)\n                tasks.append(task)\n\n            # Execute all tasks in parallel with concurrency limit\n            results = await self.gather_with_concurrency(self.max_concurrent_agents, *tasks)\n\n            # Flatten results into output list\n            for res in results:\n                if res:\n                    output.append(res)\n\n            if not has_execution_tool and len(response.text) &gt; 0:\n                 output.append(Message(type=\"assistant\", content=response.text))\n                 return output\n\n            current_context = current_context + output\n            iteration += 1\n            from_prev_iteration = True\n\n        raise Exception(\"Max iterations reached\")\n\n    async def execute_and_handle_result(self, user_id: str, iter_context: Context, function_call: LLMFunction, context: Context):\n        try:\n            result = await self.agent_execution(user_id, iter_context, function_call)\n\n            if isinstance(result, Message):\n                return result\n            elif isinstance(result, ThoughtResponse):\n                context.thoughts.append(result.thought)\n                return Message(type=\"assistant\", content=f\"Thought: {result.thought}\")\n            elif isinstance(result, VoidResponse) or result is None:\n                return Message(type=\"user\", content=f\"Agent {function_call.name} executed successfully.\")\n            elif isinstance(result, KeyStoreResponse):\n                return Message(type=\"user\", content=f\"Agent {function_call.name} executed and stored {result.description} in the store with key {result.key}.\")\n            else:\n                return Message(type=\"user\", content=f\"Result of agent {function_call.name} execution: {result}\")\n        except Exception as e:\n             return Message(type=\"user\", content=f\"Error executing agent {function_call.name}: {e}\")\n</code></pre>"},{"location":"agents/react/#agentswarm.agents.ReActAgent.gather_with_concurrency","title":"<code>gather_with_concurrency(n, *tasks)</code>  <code>async</code>","text":"<p>Runs tasks with a concurrency limit of n.</p> Source code in <code>src/agentswarm/agents/react_agent.py</code> <pre><code>async def gather_with_concurrency(self, n, *tasks):\n    \"\"\"\n    Runs tasks with a concurrency limit of n.\n    \"\"\"\n    semaphore = asyncio.Semaphore(n)\n\n    async def sem_task(task):\n        async with semaphore:\n            return await task\n\n    return await asyncio.gather(*(sem_task(task) for task in tasks))\n</code></pre>"},{"location":"core/context/","title":"Context","text":"<p>The <code>Context</code> object is the \"blood\" of the agent system. It flows through execution steps, carrying all necessary state, configuration, and history.</p> <p>Crucially, Agentswarm treats Context as immutable and isolated regarding execution flow. When an agent forks or recurses, it typically operates on a copy of the context, ensuring that its local state changes (like thought processes or temporary variables) do not pollute the parent context or parallel execution branches unless explicitly merged back (usually via the Store).</p>"},{"location":"core/context/#key-components","title":"Key Components","text":"<p>A Context instance encapsulates:</p> <ol> <li>Messages: The conversation history (User messages, System prompts, Assistant replies).</li> <li>Store: A reference to the shared data store (see Store).</li> <li>Tracing: A reference to the tracing mechanism (see Tracing).</li> <li>Thoughts: Internal reasoning steps generated by the agent during its execution.</li> <li>Trace ID / Step ID: Identifiers for observability and debugging.</li> </ol>"},{"location":"core/context/#api-reference","title":"API Reference","text":""},{"location":"core/context/#agentswarm.datamodels.Context","title":"<code>agentswarm.datamodels.Context</code>","text":"<p>The Context class contains all the information of the current context, with the messages, store, usage and so on. Moreover, the Context contains informations about the tracing and current execution step.</p> Source code in <code>src/agentswarm/datamodels/context.py</code> <pre><code>class Context():\n    \"\"\"\n    The Context class contains all the information of the current context, with the messages, store, usage and so on.\n    Moreover, the Context contains informations about the tracing and current execution step.\n    \"\"\"\n\n    # The trace_id is the unique identifier of the current trace.\n    trace_id: str\n    # The step_id is the unique identifier of the current step, inside the trace\n    step_id: str\n    # The parent_step_id is the unique identifier of the parent step, that originally creates this step\n    parent_step_id: Optional[str]\n    # The list of the messages of the current context\n    messages: List[Message]\n    # Reference to the current store\n    store: Store\n    # List of the thoughts generated in the current context by LLMs\n    thoughts: list[str]\n    # Total (current) usage of the context stack\n    usage: list[LLMUsage]\n    # Default LLM to use for the current context\n    default_llm: Optional[LLM]\n    # Reference to the tracing system\n    tracing: Tracing\n\n\n    def __init__(\n        self,\n        trace_id: str,\n        messages: List[Message],\n        store: Store,\n        tracing: Tracing,\n        thoughts: list[str] = [],\n        step_id: str = None,\n        parent_step_id: str = None,\n        default_llm: Optional[LLM] = None,\n        usage: Optional[list[LLMUsage]] = None,\n    ):\n        self.trace_id = trace_id\n        self.step_id = step_id if step_id else str(uuid.uuid4())\n        self.parent_step_id = parent_step_id\n        self.messages = messages\n        self.store = store\n        self.thoughts = thoughts\n        self.default_llm = default_llm\n        self.tracing = tracing\n        self.usage = usage if usage else []\n\n    def copy_for_execution(self):\n        \"\"\"\n        Copy the current context for a new (clean) execution.\n        The new context will have a cleaned messages list and thoughts, and will have a new step_id.\n        The parent_step_id of the new context will be the current step_id, in order to trace the execution hierarchy.\n\n        The store and the default_llm will remain the same.\n        \"\"\"\n        new_context = Context(\n            trace_id=self.trace_id,\n            messages=[],\n            store=self.store,\n            thoughts=[],\n            parent_step_id=self.step_id,\n            default_llm=self.default_llm,\n            tracing=self.tracing,\n            usage=self.usage\n        )\n        return new_context\n\n    def copy_for_iteration(self, step_id: str, messages: List[Message]):\n        \"\"\"\n        Copy the current context for a new iteration with the specified step_id and messages.\n        The new context will have the same messages and thoughts.\n        The parent_step_id of the new context will be the current step_id, in order to trace the iteration hierarchy.\n\n        The store and the default_llm will remain the same.\n        \"\"\"\n        iter_context = Context(\n            trace_id=self.trace_id,\n            messages=messages,\n            store=self.store,\n            thoughts=self.thoughts,\n            step_id=step_id,\n            parent_step_id=self.step_id,\n            default_llm=self.default_llm,\n            tracing=self.tracing,\n            usage=self.usage\n        )\n        return iter_context\n\n    def add_usage(self, usage: LLMUsage):\n        \"\"\"\n        Add usage to the current context\n        \"\"\"\n        self.usage.append(usage)\n\n    def debug_print(self) -&gt; str:\n        str_len = 100\n        output = f\"Messages ({len(self.messages)}):\\n\"\n        for idx, message in enumerate(self.messages):\n            content = message.content.replace('\\n', ' ')\n            if len(content) &gt; str_len:\n                content = content[:(str_len-3)] + \"...\"\n            len_content = str_len -len(f\"[{idx}] {message.type.upper()} \")\n            output += f\"[{idx}] {message.type.upper()} {'-'*len_content}\\n\"\n            output += f\"{content}\\n\"\n            output += f\"{'-'*str_len}\\n\"\n\n        if self.store is not None and len(self.store) &gt; 0:\n            output += f\"\\nStore ({len(self.store)}):\\n\"\n            output += f\"{'-'*str_len}\\n\"\n            for key, value in self.store.items():\n                content = str(value).replace('\\n', ' ')\n                if len(content) &gt; str_len:\n                    content = content[:(str_len-3)] + \"...\"\n                output += f\"{key}: {content}\\n\"\n            output += f\"{'-'*str_len}\\n\"\n        else:\n            output += f\"\\nStore (0):\\n\"\n            output += f\"{'-'*str_len}\\n\"\n            output += \"Empty\\n\"\n            output += f\"{'-'*str_len}\\n\"\n\n        if self.thoughts is not None and len(self.thoughts) &gt; 0:\n            output += f\"\\nThoughts:\\n\"\n            output += f\"{'-'*str_len}\\n\"\n            for thought in self.thoughts:\n                output += f\"\ud83d\udcad {thought}\\n\"\n            output += f\"{'-'*str_len}\\n\"\n\n        return output\n</code></pre>"},{"location":"core/context/#agentswarm.datamodels.Context.add_usage","title":"<code>add_usage(usage)</code>","text":"<p>Add usage to the current context</p> Source code in <code>src/agentswarm/datamodels/context.py</code> <pre><code>def add_usage(self, usage: LLMUsage):\n    \"\"\"\n    Add usage to the current context\n    \"\"\"\n    self.usage.append(usage)\n</code></pre>"},{"location":"core/context/#agentswarm.datamodels.Context.copy_for_execution","title":"<code>copy_for_execution()</code>","text":"<p>Copy the current context for a new (clean) execution. The new context will have a cleaned messages list and thoughts, and will have a new step_id. The parent_step_id of the new context will be the current step_id, in order to trace the execution hierarchy.</p> <p>The store and the default_llm will remain the same.</p> Source code in <code>src/agentswarm/datamodels/context.py</code> <pre><code>def copy_for_execution(self):\n    \"\"\"\n    Copy the current context for a new (clean) execution.\n    The new context will have a cleaned messages list and thoughts, and will have a new step_id.\n    The parent_step_id of the new context will be the current step_id, in order to trace the execution hierarchy.\n\n    The store and the default_llm will remain the same.\n    \"\"\"\n    new_context = Context(\n        trace_id=self.trace_id,\n        messages=[],\n        store=self.store,\n        thoughts=[],\n        parent_step_id=self.step_id,\n        default_llm=self.default_llm,\n        tracing=self.tracing,\n        usage=self.usage\n    )\n    return new_context\n</code></pre>"},{"location":"core/context/#agentswarm.datamodels.Context.copy_for_iteration","title":"<code>copy_for_iteration(step_id, messages)</code>","text":"<p>Copy the current context for a new iteration with the specified step_id and messages. The new context will have the same messages and thoughts. The parent_step_id of the new context will be the current step_id, in order to trace the iteration hierarchy.</p> <p>The store and the default_llm will remain the same.</p> Source code in <code>src/agentswarm/datamodels/context.py</code> <pre><code>def copy_for_iteration(self, step_id: str, messages: List[Message]):\n    \"\"\"\n    Copy the current context for a new iteration with the specified step_id and messages.\n    The new context will have the same messages and thoughts.\n    The parent_step_id of the new context will be the current step_id, in order to trace the iteration hierarchy.\n\n    The store and the default_llm will remain the same.\n    \"\"\"\n    iter_context = Context(\n        trace_id=self.trace_id,\n        messages=messages,\n        store=self.store,\n        thoughts=self.thoughts,\n        step_id=step_id,\n        parent_step_id=self.step_id,\n        default_llm=self.default_llm,\n        tracing=self.tracing,\n        usage=self.usage\n    )\n    return iter_context\n</code></pre>"},{"location":"core/store/","title":"Store","text":"<p>The <code>Store</code> is a fundamental abstraction in Agentswarm that decouples state persistence from agent logic. It allows agents to share data (like task results, intermediate variables, or long-term memory) without knowing how or where that data is stored.</p>"},{"location":"core/store/#the-abstract-concept","title":"The Abstract Concept","text":"<p>The <code>Store</code> class defines a simple key-value interface. By designing your agents to rely on this interface, you make them portable and adaptable to different environments.</p> <ul> <li>Abstraction: Your agent simply calls <code>store.set(\"key\", value)</code> or <code>store.get(\"key\")</code>.</li> <li>Flexibility: In a local script, this might just write to a Python dictionary. In a production cloud deployment, the underlying implementation could speak to Redis, a SQL database, or a cloud bucket.</li> </ul>"},{"location":"core/store/#agentswarm.datamodels.Store","title":"<code>agentswarm.datamodels.Store</code>","text":"<p>               Bases: <code>ABC</code></p> <p>The Store class defines a simple key/value API to access the store. The implementation can vary from a local dictionary, to a distributed remote store.</p> Source code in <code>src/agentswarm/datamodels/store.py</code> <pre><code>class Store(ABC):\n    \"\"\"\n    The Store class defines a simple key/value API to access the store.\n    The implementation can vary from a local dictionary, to a distributed remote store.\n    \"\"\"\n\n    @abstractmethod\n    def get(self, key: str) -&gt; any:\n        \"\"\"\n        Obtains the value associated with the given key.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def set(self, key: str, value: any):\n        \"\"\"\n        Sets the value associated with the given key.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def has(self, key: str) -&gt; bool:\n        \"\"\"\n        Checks if the store has a value associated with the given key.\n        \"\"\"\n        raise NotImplementedError\n\n    def items(self) -&gt; dict[str, any]:\n        \"\"\"\n        Returns all key-value pairs in the store.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"core/store/#agentswarm.datamodels.Store.get","title":"<code>get(key)</code>  <code>abstractmethod</code>","text":"<p>Obtains the value associated with the given key.</p> Source code in <code>src/agentswarm/datamodels/store.py</code> <pre><code>@abstractmethod\ndef get(self, key: str) -&gt; any:\n    \"\"\"\n    Obtains the value associated with the given key.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"core/store/#agentswarm.datamodels.Store.has","title":"<code>has(key)</code>  <code>abstractmethod</code>","text":"<p>Checks if the store has a value associated with the given key.</p> Source code in <code>src/agentswarm/datamodels/store.py</code> <pre><code>@abstractmethod\ndef has(self, key: str) -&gt; bool:\n    \"\"\"\n    Checks if the store has a value associated with the given key.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"core/store/#agentswarm.datamodels.Store.items","title":"<code>items()</code>","text":"<p>Returns all key-value pairs in the store.</p> Source code in <code>src/agentswarm/datamodels/store.py</code> <pre><code>def items(self) -&gt; dict[str, any]:\n    \"\"\"\n    Returns all key-value pairs in the store.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"core/store/#agentswarm.datamodels.Store.set","title":"<code>set(key, value)</code>  <code>abstractmethod</code>","text":"<p>Sets the value associated with the given key.</p> Source code in <code>src/agentswarm/datamodels/store.py</code> <pre><code>@abstractmethod\ndef set(self, key: str, value: any):\n    \"\"\"\n    Sets the value associated with the given key.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"core/store/#custom-implementations","title":"Custom Implementations","text":"<p>You are encouraged to create your own Store implementations for your specific needs. For example, if you need persistence across reboots, you might implement a <code>FileStore</code> or a <code>RedisStore</code>.</p>"},{"location":"core/store/#example-creating-a-redis-store","title":"Example: Creating a Redis Store","text":"<pre><code>from agentswarm.datamodels import Store\nimport redis\n\nclass RedisStore(Store):\n    def __init__(self, host='localhost', port=6379, db=0):\n        self.r = redis.Redis(host=host, port=port, db=db)\n\n    def get(self, key: str) -&gt; any:\n        return self.r.get(key)\n\n    def set(self, key: str, value: any):\n        self.r.set(key, value)\n\n    def has(self, key: str) -&gt; bool:\n        return self.r.exists(key)\n\n    def items(self) -&gt; dict:\n        # Implementation to fetch all keys...\n        pass\n</code></pre>"},{"location":"core/store/#local-store","title":"Local Store","text":"<p>The library comes with a ready-to-use <code>LocalStore</code> which implements the interface using an in-memory dictionary. This is perfect for testing, scripts, and single-instance applications where persistence is not required.</p>"},{"location":"core/store/#agentswarm.datamodels.LocalStore","title":"<code>agentswarm.datamodels.LocalStore</code>","text":"<p>               Bases: <code>Store</code></p> <p>The LocalStore class implements a simple key-value store in memory.</p> Source code in <code>src/agentswarm/datamodels/local_store.py</code> <pre><code>class LocalStore(Store):\n    \"\"\"\n    The LocalStore class implements a simple key-value store in memory.\n    \"\"\"\n    def __init__(self):\n        self.store = {}\n\n    def get(self, key: str) -&gt; any:\n        return self.store.get(key)\n\n    def set(self, key: str, value: any):\n        self.store[key] = value\n\n    def has(self, key: str) -&gt; bool:\n        return key in self.store\n\n    def items(self) -&gt; dict[str, any]:\n        return self.store\n</code></pre>"},{"location":"core/tracing/","title":"Tracing","text":"<p>Tracing is the observability layer of Agentswarm. It allows you to peer inside the \"black box\" of agent execution, monitoring not just final outputs but the internal reasoning, tool calls, and state changes.</p>"},{"location":"core/tracing/#the-abstract-concept","title":"The Abstract Concept","text":"<p>The <code>Tracing</code> class is an abstract contract (interface). The core framework makes calls to this interface at key moments: - When an agent starts execution. - When an agent finishes or errors. - When a loop step (like a ReAct iteration) begins. - When a tool result is received.</p> <p>By implementing this interface, you can route these signals to any observability backend you prefer.</p>"},{"location":"core/tracing/#agentswarm.utils.tracing.Tracing","title":"<code>agentswarm.utils.tracing.Tracing</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>src/agentswarm/utils/tracing.py</code> <pre><code>class Tracing(ABC):\n    @abstractmethod\n    def trace_agent(context: Context, agent_id: str, arguments: dict):\n        pass\n\n    @abstractmethod\n    def trace_loop_step(context: Context, step_name: str):\n        pass\n\n    @abstractmethod\n    def trace_agent_result(context: Context, agent_id: str, result: Any):\n        pass\n\n    @abstractmethod\n    def trace_agent_error(context: Context, agent_id: str, error: Exception):\n        pass\n</code></pre>"},{"location":"core/tracing/#extensibility","title":"Extensibility","text":"<p>You are not limited to local files. You can implement a <code>Tracing</code> subclass to send data to: - Datadog / New Relic: For enterprise monitoring. - LangSmith / Langfuse: For LLM-specific observability. - Console / Stdout: For real-time streaming to the terminal.</p>"},{"location":"core/tracing/#example-console-tracer","title":"Example: Console Tracer","text":"<pre><code>from agentswarm.utils.tracing import Tracing\nfrom agentswarm.datamodels import Context\n\nclass ConsoleTracing(Tracing):\n    def trace_agent(self, context: Context, agent_id: str, arguments: dict):\n        print(f\"\ud83d\ude80 Starting Agent: {agent_id} with inputs: {arguments}\")\n\n    def trace_agent_result(self, context: Context, agent_id: str, result: any):\n        print(f\"\u2705 Agent {agent_id} completed: {result}\")\n\n    # ... implement other methods ...\n</code></pre>"},{"location":"core/tracing/#local-tracing","title":"Local Tracing","text":"<p>The provided <code>LocalTracing</code> implementation writes trace events to JSON files on the local filesystem. This is the default used in examples and allows for post-execution analysis (e.g., using the included trace viewer).</p>"},{"location":"core/tracing/#agentswarm.utils.tracing.LocalTracing","title":"<code>agentswarm.utils.tracing.LocalTracing</code>","text":"<p>               Bases: <code>Tracing</code></p> Source code in <code>src/agentswarm/utils/tracing.py</code> <pre><code>class LocalTracing(Tracing):\n    def __init__(self, trace_path: str = './traces'):\n        self.trace_path = trace_path\n\n    def trace_agent(self, context: Context, agent_id: str, arguments: dict):\n        os.makedirs(self.trace_path, exist_ok=True)\n        with open(os.path.join(self.trace_path, f\"{context.trace_id}.json\"), \"a\") as f:\n            trace_data = {\n                \"timestamp\": datetime.now().isoformat(),\n                \"type\": \"agent\",\n                \"step_id\": context.step_id,\n                \"parent_step_id\": context.parent_step_id,\n                \"agent_id\": agent_id,\n                \"arguments\": arguments,\n                \"messages\": [message.model_dump() for message in context.messages],\n                \"store\": _get_store_snapshot(context.store),\n                \"thoughts\": context.thoughts\n            }\n            f.write(json.dumps(trace_data) + \"\\n\")\n\n    def trace_loop_step(self, context: Context, step_name: str):\n        os.makedirs(self.trace_path, exist_ok=True)\n        with open(os.path.join(self.trace_path, f\"{context.trace_id}.json\"), \"a\") as f:\n            trace_data = {\n                \"timestamp\": datetime.now().isoformat(),\n                \"type\": \"loop_step\",\n                \"step_id\": context.step_id,\n                \"parent_step_id\": context.parent_step_id,\n                \"agent_id\": step_name, # Use agent_id field to store the step name for UI compatibility\n                \"arguments\": {},\n                \"messages\": [message.model_dump() for message in context.messages],\n                \"store\": _get_store_snapshot(context.store),\n                \"thoughts\": context.thoughts\n            }\n            f.write(json.dumps(trace_data) + \"\\n\")\n\n    def trace_agent_result(self, context: Context, agent_id: str, result: Any):\n        os.makedirs(self.trace_path, exist_ok=True)\n        with open(os.path.join(self.trace_path, f\"{context.trace_id}.json\"), \"a\") as f:\n\n            serialized_result = None\n            try:\n                if hasattr(result, 'model_dump'):\n                    serialized_result = result.model_dump(mode='json')\n                elif hasattr(result, 'dict'):\n                    serialized_result = result.dict()\n                elif isinstance(result, list):\n                    serialized_result = []\n                    for item in result:\n                        if hasattr(item, 'model_dump'):\n                            serialized_result.append(item.model_dump(mode='json'))\n                        elif hasattr(item, 'dict'):\n                            serialized_result.append(item.dict())\n                        else:\n                            serialized_result.append(str(item))\n                else:\n                    serialized_result = str(result)\n            except Exception:\n                serialized_result = str(result)\n\n            trace_data = {\n                \"timestamp\": datetime.now().isoformat(),\n                \"type\": \"agent_result\",\n                \"step_id\": context.step_id,\n                \"parent_step_id\": context.parent_step_id,\n                \"agent_id\": agent_id,\n                \"result\": serialized_result,\n                \"messages\": [message.model_dump() for message in context.messages],\n                \"store\": _get_store_snapshot(context.store),\n                \"thoughts\": context.thoughts\n            }\n            f.write(json.dumps(trace_data) + \"\\n\")\n\n    def trace_agent_error(self, context: Context, agent_id: str, error: Exception):\n        os.makedirs(self.trace_path, exist_ok=True)\n        with open(os.path.join(self.trace_path, f\"{context.trace_id}.json\"), \"a\") as f:\n            trace_data = {\n                \"timestamp\": datetime.now().isoformat(),\n                \"type\": \"agent_error\",\n                \"step_id\": context.step_id,\n                \"parent_step_id\": context.parent_step_id,\n                \"agent_id\": agent_id,\n                \"error\": str(error),\n                \"messages\": [message.model_dump() for message in context.messages],\n                \"store\": _get_store_snapshot(context.store),\n                \"thoughts\": context.thoughts\n            }\n            f.write(json.dumps(trace_data) + \"\\n\")\n</code></pre>"},{"location":"llms/","title":"Large Language Models (LLMs)","text":"<p>The <code>LLM</code> class is the interface between Agentswarm and the various AI model providers.</p>"},{"location":"llms/#the-abstract-concept","title":"The Abstract Concept","text":"<p>Agentswarm is model-agnostic. The <code>LLM</code> abstract base class defines a standard way to: 1.  Generate text: Given a list of messages. 2.  Handle Tools: define function definitions (schemas) and parse tool calls from the model's response.</p> <p>By implementing this interface, you can add support for any model provider that supports function calling (or even emulate it).</p>"},{"location":"llms/#agentswarm.llms.LLM","title":"<code>agentswarm.llms.LLM</code>","text":"Source code in <code>src/agentswarm/llms/llm.py</code> <pre><code>class LLM:\n    async def generate(self, messages: List[Message], functions: List[LLMFunction] = None) -&gt; LLMOutput:\n        pass\n</code></pre>"},{"location":"llms/#implementing-a-custom-provider","title":"Implementing a Custom Provider","text":"<p>To add a new provider (e.g., Anthropic, OpenAI, local Llama), create a subclass of <code>LLM</code> and implement <code>generate</code>.</p> <pre><code>from agentswarm.datamodels import Message, LLMFunction\nfrom agentswarm.llms import LLM, LLMOutput\n\nclass MyCustomLLM(LLM):\n    def __init__(self, api_key: str, model_name: str = \"gpt-4\"):\n        self.client = ... # Initialize your client\n        self.model = model_name\n\n    async def generate(self, messages: list[Message], functions: list[LLMFunction] = None) -&gt; LLMOutput:\n        # 1. Convert Agentswarm Messages to provider format\n        # 2. Convert LLMFunctions to provider tool schemas\n        # 3. Call the API\n        # 4. Parse the response into LLMOutput (text + function_calls)\n        pass\n</code></pre>"},{"location":"llms/#supported-providers","title":"Supported Providers","text":"<p>Agentswarm currently includes support for Gemini.</p>"},{"location":"llms/#gemini","title":"Gemini","text":"<p>The <code>GeminiLLM</code> implementation connects to Google's Vertex AI or Generative AI SDKs.</p>"},{"location":"llms/#agentswarm.llms.GeminiLLM","title":"<code>agentswarm.llms.GeminiLLM</code>","text":"<p>               Bases: <code>LLM</code></p> Source code in <code>src/agentswarm/llms/gemini.py</code> <pre><code>class GeminiLLM(LLM):\n\n    def __init__(self, api_key: str = None, model: str = 'gemini-3-flash-preview', client: Client = None):\n        if api_key is None and client is None:\n            raise ValueError(\"api_key or client must be provided\")\n        self.client = client if client is not None else Client(api_key=api_key)\n        self.model = model\n\n    async def generate(self, messages: List[Message], functions: List[LLMFunction] = None) -&gt; LLMOutput:\n        contents = []\n        sys_instruct = []\n        for message in messages:\n            if message.type != 'system':\n                role = 'model' if message.type == 'assistant' else message.type\n                contents.append(types.Content(\n                    role=role,\n                    parts=[types.Part(text=message.content)]))\n            else:\n                sys_instruct.append(message.content)       \n        if len(sys_instruct) == 0:\n            sys_instruct = None\n\n        function_declarations = []\n        if functions is not None:\n            for fn in functions:\n                function_declarations.append({\n                    \"name\": fn.name,\n                    \"description\": fn.description,\n                    \"parameters\": fn.parameters\n                })\n            tools = [types.Tool(function_declarations=function_declarations)]\n        else:\n            tools = None\n\n        response = await self.client.aio.models.generate_content(\n            model=self.model,\n            config=types.GenerateContentConfig(\n                temperature=0,\n                tools=tools,\n                system_instruction=sys_instruct,\n                safety_settings = [types.SafetySetting(\n                    category=\"HARM_CATEGORY_HATE_SPEECH\",\n                    threshold=\"OFF\"\n                    ),types.SafetySetting(\n                    category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n                    threshold=\"OFF\"\n                    ),types.SafetySetting(\n                    category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n                    threshold=\"OFF\"\n                    ),types.SafetySetting(\n                    category=\"HARM_CATEGORY_HARASSMENT\",\n                    threshold=\"OFF\"\n                    )\n                ],\n            ),\n            contents=contents\n        )\n\n        usg = response.usage_metadata\n        usage = LLMUsage(\n            model=self.model,\n            prompt_token_count=usg.prompt_token_count if usg.prompt_token_count is not None else 0,\n            thoughts_token_count=usg.thoughts_token_count if usg.thoughts_token_count is not None else 0,\n            tool_use_prompt_token_count=usg.tool_use_prompt_token_count if usg.tool_use_prompt_token_count is not None else 0,\n            candidates_token_count=usg.candidates_token_count if usg.candidates_token_count is not None else 0,\n            total_token_count=usg.total_token_count if usg.total_token_count is not None else 0\n        )\n\n        output_function_calls = []\n        text_parts = []\n        if response.candidates and response.candidates[0].content and response.candidates[0].content.parts:\n            for part in response.candidates[0].content.parts:\n                if part.text:\n                    text_parts.append(part.text)\n                if part.function_call:\n                    args = part.function_call.args\n                    if args is not None and not isinstance(args, dict):\n                        try:\n                            args = dict(args)\n                        except Exception:\n                            pass\n                    output_function_calls.append(LLMFunctionExecution(name=part.function_call.name, arguments=args))\n\n        return LLMOutput(text=\"\".join(text_parts), function_calls=output_function_calls, usage=usage)\n</code></pre>"}]}